configfile: "config/snake_config.yaml"

import time
import os
import csv

sample_names=list(config['samples'].keys())

include:
    "rules/qc.smk"
include:
    "rules/trimming.smk"
include:
    "rules/alignment.smk"
include:
    "rules/expression.smk"
#include:
#    "rules/dge_analysis.smk"

rule all:
    input:
        "results/read_quality/QC_raw/qcreport_raw.html",
#        "results/read_quality/QC_trimmed/qcreport_trimmed.html",
        expand("results/expression/{sample}/compare.stats", sample=sample_names),
        expand("results/preprocessed/{sample}_val_1.fq.gz", sample=sample_names),
        expand("results/alignment/star/{sample}_Aligned.sortedByCoord.out.bam", sample=sample_names),
        expand("results/expression/htseq/{sample}_gene_counts.txt", sample=sample_names),
        expand("results/expression/featurecounts/{sample}_gene_counts.tsv", sample=sample_names),
        expand("results/expression/salmon/{sample}/quant.sf", sample=sample_names)

rule qc_bench:
    input:
        "results/read_quality/QC_raw/qcreport_raw.html",
        "results/read_quality/QC_trimmed/qcreport_trimmed.html"

rule trim_bench:
    input: 
        expand("results/preprocessed/{sample}_val_1.fq.gz", sample=sample_names),
        expand("results/preprocessed/{sample}_R1.trimmed.fastq.gz", sample=sample_names)

rule alignment_bench:
    input:
        expand("results/alignment/{sample}.bam", sample=sample_names),
        expand("results/alignment/star/{sample}_Aligned.sortedByCoord.out.bam", sample=sample_names)

rule expression_bench:
    input:
#        expand("results/expression/{sample}/compare.stats", sample=sample_names),
        expand("results/expression/htseq/{sample}_gene_counts.txt", sample=sample_names),
        expand("results/expression/featurecounts/{sample}_gene_counts.tsv", sample=sample_names),
        expand("results/expression/salmon/{sample}/quant.sf", sample=sample_names)

#rule all_dge_analysis:

def compile_benchmarks(benchmark_fp: str, stats_fp: str):
    """Aggregate all the benchmark files into one and put it in stats_fp. Modified from Ulthran (ctbushman@gmail.com) on Github"""
    benchmarks = []
    # Recursively find all files in the benchmark_fp directory and its subdirectories
    for root, dirs, files in os.walk(benchmark_fp):
        for file in files:
            benchmarks.append(os.path.join(root, file))

    if not benchmarks:
        print("No benchmark files found")
        return None
    headers = ["rule"]
    with open(benchmarks[0], "r") as f:
        reader = csv.reader(f, delimiter="\t")
        headers += next(reader)

    if not os.path.exists(stats_fp):
        os.makedirs(stats_fp)
    stats_file = os.path.join(
        stats_fp,
        f"{time.strftime("%Y%m%d-%H%M%S")}_benchmarks.tsv",
    )
    with open(stats_file, "w") as f:
        writer = csv.writer(f, delimiter="\t")
        writer.writerow(headers)
        for fp in benchmarks:
            with open(fp, "r") as g:
                reader = csv.reader(g, delimiter="\t")
                next(reader)  # Headers line
                for row in reader:
                    writer.writerow([fp[20:-4]] + row)
    print(f"Benchmarks aggregated at {stats_file}")


def compile_logs(log_fp: str, stats_fp: str):
    """Aggregate all the log files into one and put it in stats_fp. Modified from Ulthran (ctbushman@gmail.com) on Github"""
    logs = []
    # Recursively find all files in the log_fp directory and its subdirectories
    for root, dirs, files in os.walk(log_fp):
        for file in files:
            logs.append(os.path.join(root, file))

    if not logs:
        print("No log files found")
        return None

    if not os.path.exists(stats_fp):
        os.makedirs(stats_fp)
    stats_file = os.path.join(
        stats_fp,
        f"{time.strftime("%Y%m%d-%H%M%S")}_logs.log",
    )
    with open(stats_file, "w") as f:
        for fp in logs:
            with open(fp, "r") as g:
                f.write(f"{fp[14:-4]}\n")
                f.write(g.read())
                f.write("\n\n")
    print(f"Logs aggregated at {stats_file}")

def remove_benchmarks_and_logs(benchmark_fp: str, log_fp: str):
    for root, dirs, files in os.walk(log_fp):
        for file in files:
            if file.endswith(".log"):
                os.remove(os.path.join(root, file))
    for root, dirs, files in os.walk(benchmark_fp):
        for file in files:
            if file.endswith(".tsv"):
                os.remove(os.path.join(root, file))
    print("Removed old logs and benchmarks")

onstart:
    remove_benchmarks_and_logs("workflow/benchmarks", "workflow/logs")

onsuccess:
    compile_benchmarks("workflow/benchmarks", "workflow/stats")
    compile_logs("workflow/logs", "workflow/stats")

onerror:
    compile_benchmarks("workflow/benchmarks", "workflow/stats")
    compile_logs("workflow/logs", "workflow/stats")

